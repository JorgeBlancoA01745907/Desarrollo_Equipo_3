As one of the branches of machine learning, the deep learning model combined with artificial intelligence is widely used in the field of computer vision technology. The image recognition field, represented by medical image analysis, is also developing. Its advantage is that it does not rely on human annotation, allowing the computer to recognize and process feature information omitted by humans during the model training process, achieving or even exceeding the accuracy of human processing. Based on the general lack of explainability caused by the unknown data processing process in the deep model, the existing solutions mainly include the establishment of internal explainability, attention mechanism interpretation of specific models, and the interpretation of unknowable models represented by LIME. The way to quantitatively assess interpretability is still being explored, especially in the interpretative assessment of both doctors and patients in medical decision-related models, with several scales proposed for reference. The current research on the application of artificial intelligence deep learning models in medical imaging generally pays more attention to accuracy rather than explainability, resulting in a lack of explainability and thus hindering the practical clinical application of deep learning models. Therefore, the need to analyze the development of medical image analysis in the field of artificial intelligence and computer vision technology, and how to balance accuracy and interpretability to develop deep learning models that both doctors and patients can trust, will become the research focus of the industry in the future.